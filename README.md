# Cross-lingual-question-system

1. Project Overview

This project aims to build and train a sophisticated Question Answering (QA) model capable of understanding and extracting answers from text across multiple languages. The system leverages a powerful multilingual transformer model fine-tuned on the Typologically Diverse Question Answering (TyDi QA) dataset.

The core task is Extractive Question Answering, where the model is given a context (a paragraph) and a question, and its goal is to identify and extract the exact span of text from the context that answers the question.

2. Dataset

We use the TyDi QA dataset, specifically the primary_task. This dataset is designed for building multilingual QA systems and includes questions and answers in 11 typologically diverse languages, such as Arabic, English, Finnish, Russian, and more.

Dataset: tydiqa

Configuration: primary_task

The script is configured to load a small subset of the data for quick experimentation and debugging purposes.

3. Model Architecture

The core of this system is the xlm-roberta-base model. This is a multilingual transformer model pre-trained on 100 different languages. Its deep understanding of cross-lingual patterns makes it an ideal choice for this task.

We use the AutoModelForQuestionAnswering head from the Hugging Face Transformers library, which adds a QA-specific layer on top of the base model. This layer is responsible for predicting the start and end positions of the answer span.

4. Technologies Used

Programming Language: Python

Core Libraries:

PyTorch

Hugging Face transformers (for models and tokenizers)

Hugging Face datasets (for data loading and processing)

NumPy

5. How to Run the Project

The entire logic for data loading, preprocessing, training, and testing is contained within the qa_trainer.py script.

Prerequisites

Ensure you have the necessary libraries installed:

pip install torch datasets transformers


Note: The script is designed to run in an environment with a GPU (CUDA) available to leverage fp16 for faster training.

Running the Script

Execute the qa_trainer.py script.

The script will automatically:

Download a sample of the TyDi QA dataset.

Load the xlm-roberta-base model and tokenizer.

Preprocess the data, which involves complex logic to map character-level answers to token-level start and end positions.

Fine-tune the model on the training data for one epoch.

Test the trained model on a sample from the validation set and display the predicted answer versus the actual answer.

Important Note on Performance

The script is currently configured to run on a small sample of the data (1000 training examples) for a single epoch. This is done for demonstration and quick execution.

Disclaimer: The answers generated by this quick training run may not be accurate. For high-quality, reliable results, the model must be trained on a much larger dataset (e.g., the full training set) and for more epochs (e.g., 3-5).

6. Example Output

After training, the script will output a comparison like the following:

[Context]: A long paragraph in one of the 11 languages...

[Question]: A question about the context.

[Predicted Answer]: The model's extracted answer from the context.

[Actual Answer]: The ground-truth answer from the dataset.
